1、单机安装
    下载 kafka 集成包，不要源码包。 https://kafka.apache.org/downloads

    # 配置 kafka
    mkdir /opt/kafka_2.12-3.2.0
    cd /opt/kafka_2.12-3.2.0
    vim config/server.properties
        listeners = PLAINTEXT://127.0.0.1:9092
        log.dirs=/opt/kafka_2.12-3.2.0/kafka-logs # 可以选择不改
        zookeeper.connect=127.0.0.1:2181

    # 启动 zookeeper
    ./bin/zookeeper-server-start.sh config/zookeeper.properties

    # 启动 kafka
    ./bin/kafka-server-start.sh -daemon ./config/server.properties

    # 验证kafka是否启动成功
    # 方法一
    ps ef | grep kakfa
    # 方法二
    jps

    # 创建 topic
    ./bin/kafka-topics.sh --create  --bootstrap-server 127.0.0.1:9092   --topic louisTest44

    # 查看当前服务器下 kafka 的 topic 列表
    ./bin/kafka-topics.sh  --bootstrap-server 127.0.0.1:9092 --list

    # 查看某个 topic 详情
    ./bin/kafka-topics.sh  --bootstrap-server 127.0.0.1:9092 --describe --topic louisTest44

    # 创建 kafka 生产者
    bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic louisTest33

    # 创建 kafka 消费者
    # --from-beginning 是从头开始消费
    bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic louisTest33 --from-beginning

    安装参考文档：https://blog.csdn.net/u013416034/article/details/123875299

    kafka官网资料：https://kafka.apache.org/intro


2、Kafka 介绍
    概念：是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，基于发布订阅模式的消息引擎系统。
         简言之就是一个消息中间件。

    Kafka 涉及术语
        Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker。
        Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。
               物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上
        Partition 分区：Partition是物理上的概念，每个Topic包含一个或多个Partition。
        批次：为了提高效率， 消息会分批次写入 Kafka，批次就代指的是一组消息。
        消息：Kafka 中的数据单元被称为消息，也被称为记录
        Producer 生产者：负责发布消息到Kafka broker。
        Consumer 消费者：消息消费者，向Kafka broker读取消息的客户端。
        Consumer Group 消费者群组:每个Consumer属于一个特定的Consumer Group
        副本：Kafka 中消息的备份又叫做 副本（Replica），副本的数量是可以配置的
             Kafka 定义了两类副本：领导者副本（Leader Replica） 和 追随者副本（Follower Replica），前者对外提供服务，后者只是被动跟随。
        Rebalance 重平衡：消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。高可用的体现。

    Producer 配置说明
        bootstrap.servers： kafka的地址。
        acks:消息的确认机制，默认值是0。
            acks=0：如果设置为0，生产者不会等待kafka的响应。
            acks=1：这个配置意味着kafka会把这条消息写到本地日志文件中，但是不会等待集群中其他机器的成功响应。
            acks=all：这个配置意味着leader会等待所有的follower同步完成。这个确保消息不会丢失，除非kafka集群中所有机器挂掉。这是最强的可用性保证。
        retries：配置为大于0的值的话，客户端会在消息发送失败时重新发送。
        batch.size:当多条消息需要发送到同一个分区时，生产者会尝试合并网络请求。这会提高client和生产者的效率。
        key.serializer: 键序列化，和Map类似。
        value.serializer:值序列化，要发送的数据，数据格式为String类型的。

    Consumer 配置说明
        bootstrap.servers： kafka的地址。
        group.id：组名 不同组名可以重复消费。例如你先使用了组名A消费了kafka的1000条数据，但是你还想再次进行消费这1000条数据，
            并且不想重新去产生，那么这里你只需要更改组名就可以重复消费了。
        enable.auto.commit：是否自动提交，默认为true。
        auto.commit.interval.ms: 从poll(拉)的回话处理时长。
        session.timeout.ms:超时时间。
        max.poll.records:一次最大拉取的条数。
        auto.offset.reset：消费规则，默认earliest。
            earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费。
            latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据。
            none: topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常。
        key.serializer: 键序列化
        value.deserializer:值序列化

    Kafka 的特性
        高吞吐、低延迟：每秒可以处理几十万条消息，它的延迟只有几毫秒。
        持久性：消息被持久化到磁盘，并支持数据备份防止数据丢失
        容错性：某个节点宕机，Kafka 集群能够正常工作
        高并发： 支持数千个客户端同时读写
        高伸缩性： 每个主题包含多个分区，主题中的分区可以分布在不同的主机(broker)中。

    Kafka 的使用场景
        用来跟踪用户行为：比如淘宝购物，将用户行为作为一个个消息传递给 Kafka ，这样就可以生成报告，可以做智能推荐，购买喜好等。
        限流削峰：某一时刻请求特别多，可以把请求写入Kafka 中，避免直接请求后端程序导致服务崩溃。

    Kafka 为何如此之快
        零拷贝
        数据记录分批发送，减少 I/O 延迟
        顺序写入磁盘的方式，避免了随机磁盘寻址的浪费
        消息压缩，snappy、gzip 和 lz4等压缩算法。涉及压缩大小，压缩时间，解压缩时间

    Kafka Producer 消息发送
        简单消息发送：不管结果如何直接发送。调用 send() 方法
        同步发送消息：发送并返回结果。首先调用 send() 方法，然后再调用 get() 方法等待 Kafka 响应。如果服务器返回错误，get() 方法会抛出异常，如果没有，会得到消息记录。
        异步发送消息：发送并回调。解决了同一时间只能有一个消息在发送，异步发送消息的同时能够对异常情况进行处理，生产者提供了回掉支持

    Kafka Producer 分区策略
        顺序分配：默认策略。消息是均匀的分配给每个 partition
        随机轮询：随机的向 partition 中保存消息。本质上还是将数据均匀地打散到各个分区
        按照 key 进行消息保存：可以保证同一个 Key 的所有消息都进入到相同的分区里面

    Kafka Consumer Offset 偏移量
        概念：它是一个不断递增的整数值，用来记录消费者发生重平衡时的位置，以便用来恢复数据。
        问题：如果提交的偏移量和客户端最后一次处理的偏移量不相等，那么位于两个偏移量之间的消息就会被重复处理
        解决办法：
            自动提交：enable.auto.commit 被设置为true。让消费者自动提交偏移量。
                      那么每过 5s，消费者会自动把从 poll() 方法轮询到的最大偏移量提交上去。
                      提交时间间隔由 auto.commit.interval.ms 控制，默认是 5s。
            异步提交：auto.commit.offset 设置为 false。使用 commitSync() 提交偏移量。
                      这个 API 会提交由 poll() 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。
                      如果发生了Rebalance，发生在再均衡之间的所有消息都将被重复处理。
            异步提交：异步提交 commitAsync() 与同步提交 commitSync() 最大的区别在于异步提交不会进行重试，同步提交会一直进行重试。
            同步和异步组合提交：在消费者关闭之前一般会组合使用 commitAsync 和 commitSync 提交偏移量


3、代码示例
    第一个例子：
        public static void testTopicKafka() {
            // topic: 消息队列的名称，可以先行在kafka服务中进行创建。如果kafka中并未创建该topic，那么便会自动创建！
            KafkaProducerTest kpt = new KafkaProducerTest("louisTest55");
            Thread thread21 = new Thread(kpt);
            thread21.start();
            KafkaConsumerTest kct = new KafkaConsumerTest("louisTest55");
            Thread thread22 = new Thread(kct);
            thread22.start();
        }

        // 生产者
        public class KafkaProducerTest implements Runnable {
            private final KafkaProducer<String, String> producer;
            private final String topic;

            public KafkaProducerTest(String topicName) {
                Properties props = new Properties();
                props.put("bootstrap.servers", "127.0.0.1:9092");
                props.put("acks", "all");
                props.put("retries", 0);
                props.put("batch.size", 16384);
                props.put("key.serializer", StringSerializer.class.getName());
                props.put("value.serializer", StringSerializer.class.getName());
                this.producer = new KafkaProducer<String, String>(props);
                this.topic = topicName;
            }

            @Override
            public void run() {
                int messageNo = 1;
                try {
                    for(;;) {
                        String messageStr = "你好，这是第" + messageNo + "条数据";
                        // 往 kafka 里推数据
                        producer.send(new ProducerRecord<String, String>(topic, "MyMessage", messageStr));
                        if (messageNo % 100 == 0) {
                            System.out.println("发送的消息：" + messageStr);
                        }
                        if (messageNo % 1000 == 0) {
                            System.out.println("成功发送了" + messageNo + "条");
                            break;
                        }
                        messageNo++;
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                } finally {
                    producer.close();
                }
            }
        }

        // 消费者
        public class KafkaConsumerTest implements Runnable {
            private final KafkaConsumer<String, String> consumer;
            private ConsumerRecords<String, String> msgList;
            private final String topic;
            private static final String GROUPID = "groupA";

            public KafkaConsumerTest(String topicName) {
                Properties props = new Properties();
                props.put("bootstrap.servers", "127.0.0.1:9092");
                props.put("group.id", GROUPID);
                props.put("enable.auto.commit", "true");
                props.put("session.timeout.ms", "30000");
                props.put("auto.offset.reset", "earliest");
                props.put("key.deserializer", StringDeserializer.class.getName());
                props.put("value.deserializer", StringDeserializer.class.getName());
                // 创建消费者
                this.consumer = new KafkaConsumer<String, String>(props);
                this.topic = topicName;
                // 订阅主题
                this.consumer.subscribe(Arrays.asList(topic));
            }

            @Override
            public void run() {
                int messageNo = 1;
                System.out.println("------- 开始消费 ---------");
                try {
                    for(;;) {
                        msgList = consumer.poll(1000);
                        if (null != msgList && msgList.count() > 0) {
                            for (ConsumerRecord<String, String> record:msgList) {
                                // 消费100条就打印 ,但打印的数据不一定是这个规律的
                                if (messageNo % 5 == 0) {
                                    System.out.println(messageNo + "======receive: key=" + record.key() + ", value=" +
                                            record.value() + ", offset=" + record.offset());
                                }
                                // 当消费了1000条就退出
                                if (messageNo % 2000 == 0) {
                                    break;
                                }
                                messageNo++;
                            }
                        } else {
                            Thread.sleep(1000);
                        }
                    }
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    consumer.close();
                }
            }
        }


